{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Info\n",
    "\n",
    "### classes\n",
    "769 Cue onset left (class 1)  <br>\n",
    "770 Cue onset right (class 2)  <br>\n",
    "771 Cue onset foot (class 3) <br>\n",
    "772 Cue onset tongue (class 4)  <br>\n",
    "\n",
    "### Data\n",
    "Input data is generally a 4D tensor with size (9, 288, 22, 1000)\n",
    "\n",
    "9 subjects <br>\n",
    "288 trials for each subject <br>\n",
    "25 channels (22 EEGs and 3 EOGs, only EEGs matter) <br>\n",
    "1000 time steps (4 seconds at 250 Hz) <br>\n",
    "\n",
    "Output data is generally a 2D matrix with size (9, 288)\n",
    "\n",
    "Each session consists of 288 trials. Each session is comprised of 6 runs separated by short breaks. One run consists of 48 trials (12 for each of the four possible classes), yielding a total of 288 trials per session. \n",
    "\n",
    "1 (session) = 288 trials = 4 (classes) * 12 (class repetitions) * 6 (runs)\n",
    "\n",
    "Two sessions on different days were recorded for each subject?\n",
    "\n",
    "288 trials of (4 seconds of active imagination + 1.5 seconds rest) for each session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Softmax\n",
    "from keras.layers import LSTM, GRU, advanced_activations\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, Conv2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if true, we train one individual subjects, if False, we train across all subjects combined.\n",
    "Process_data_each_subject = False\n",
    "\n",
    "if Process_data_each_subject == True:\n",
    "    Subject_train = 1   # which subject to train on\n",
    "\n",
    "# number of chuncks that we break the signal into. For example, if Time_devide = 10, we break each signal into 10 signals\n",
    "# with 100 time points\n",
    "Time_devide = 10\n",
    "\n",
    "# Number of trials used for testing\n",
    "Num_test=50\n",
    "\n",
    "Architecture = 4    # {1,2,3,4,5,6}\n",
    "\n",
    "epochs = 1             # Number of epochs\n",
    "N_mont = 20        # Number of iterations\n",
    "\n",
    "batch_num = 10      # number of batches (as opposed to batch size)\n",
    "\n",
    "# Run the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'project_datasets\\A01T_slice.mat', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d42b61106310>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Make Sure You are adding your directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mA01T\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'project_datasets\\A01T_slice.mat'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#A01T = h5py.File('C:\\\\Users\\\\manit\\\\Downloads\\\\Projects\\\\project_datasets\\\\A01T_slice.mat','r')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#A01T = h5py.File('A01T_slice.mat','r')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA01T\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sahba\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[0;32m    267\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m                 \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sahba\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'project_datasets\\A01T_slice.mat', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# Make Sure You are adding your directory \n",
    "A01T = h5py.File('project_datasets\\A01T_slice.mat','r')\n",
    "#A01T = h5py.File('C:\\\\Users\\\\manit\\\\Downloads\\\\Projects\\\\project_datasets\\\\A01T_slice.mat','r')\n",
    "#A01T = h5py.File('A01T_slice.mat','r')\n",
    "X = np.copy(A01T['image'])\n",
    "y = np.copy(A01T['type'])\n",
    "y = y[0,0:X.shape[0]:1]\n",
    "y = np.asarray(y, dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing (creating training, validation, and test sets)\n",
    "\n",
    "# data is saved in the project_datasets folder\n",
    "# project_datasets folder is placed in the same folder as this notebook\n",
    "\n",
    "num_class=4;\n",
    "num_subject=9;\n",
    "Data={}\n",
    "\n",
    "for i in range(num_subject):\n",
    "    address = 'project_datasets\\\\' + 'A0' + str(i+1) + 'T_slice.mat' # address where data is saved\n",
    "    #address='C:\\\\Users\\\\manit\\\\Downloads\\\\Projects\\\\project_datasets\\\\'+'A0' + str(i+1) + 'T_slice.mat'\n",
    "    #address ='A0' + str(i+1) + 'T_slice.mat'\n",
    "    A01T = {}\n",
    "    A01T = h5py.File(address, 'r')\n",
    "    Data['X'+str(i+1)] = np.copy(A01T['image'])\n",
    "    y = np.copy(A01T['type'])\n",
    "    y = y[0,0:Data['X'+str(i+1)].shape[0]:1]\n",
    "    Data['y'+str(i+1)] = np.asarray(y, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting nans\n",
    "\n",
    "for i in range(num_subject):\n",
    "    Temp1=Data['X'+str(i+1)]\n",
    "    Temp2=Data['y'+str(i+1)]\n",
    "    Index_drop=np.argwhere(np.isnan(Temp1))[:,0]\n",
    "    Index=list(set(Index_drop))\n",
    "    Data['X'+str(i+1)]=np.delete(Temp1,Index,axis=0)\n",
    "    Data['y'+str(i+1)]=np.delete(Temp2,Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating training, validation, and test sets and removing EOG channels\n",
    "# updating labels to {0,1,2,3}\n",
    "\n",
    "Train_X, Test_X = {},{}\n",
    "Train_y, Test_y = {},{}\n",
    "Data_X, Data_y = {},{}\n",
    "\n",
    "\n",
    "for i in range(num_subject):\n",
    "    \n",
    "    # getting trial indexes and shuffle them\n",
    "    index=np.arange(Data['y'+str(i+1)].shape[0])\n",
    "    np.random.shuffle(index)\n",
    "    \n",
    "    # generating training, validation, and test sets\n",
    "    test_index=index[0:Num_test]\n",
    "    train_index=index[Num_test:]   \n",
    "    \n",
    "    Data_X[i+1] = (Data['X'+str(i+1)][index,0:22])\n",
    "    Data_y[i+1] = Data['y'+str(i+1)] - 769\n",
    "    \n",
    "    Train_X[i+1]=(Data['X'+str(i+1)][train_index,0:22])\n",
    "    Train_y[i+1]=Data['y'+str(i+1)][train_index] - 769\n",
    "    \n",
    "    Test_X[i+1]=(Data['X'+str(i+1)][test_index,0:22])\n",
    "    Test_y[i+1]=Data['y'+str(i+1)][test_index] - 769\n",
    "\n",
    "    \n",
    "print(Data_X[4].shape)\n",
    "print(Train_X[4].shape)\n",
    "print(Test_X[4].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remoing mean from data for each subject\n",
    "\n",
    "if (Process_data_each_subject):\n",
    "    Mean_sub = {}     #  22 * 1000\n",
    "\n",
    "    for i in np.arange(num_subject):\n",
    "\n",
    "        Mean_sub[i+1] = np.mean(Data_X[i+1],axis = 0)   # mean across trials\n",
    "        Train_X[i+1] -= Mean_sub[i+1]                   # broadcating\n",
    "        Test_X[i+1] -= Mean_sub[i+1]\n",
    "\n",
    "    print(Mean_sub[4].shape)\n",
    "    print(Train_X[4].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# breaking data across subjects\n",
    "# changing from ? * 1000 * 22 to (?*9) * 1000 * 22\n",
    "\n",
    "if (Process_data_each_subject==False):\n",
    "    \n",
    "    X_train_all = Train_X[1]\n",
    "    X_test_all = Test_X[1]\n",
    "    X_Data_all = Data_X[1]\n",
    "\n",
    "    y_train_all = Train_y[1]\n",
    "    y_test_all = Test_y[1]\n",
    "    y_Data_all = Data_y[1]\n",
    "\n",
    "    for subject in np.arange(2,num_subject+1):\n",
    "\n",
    "        X_Data_all = np.concatenate((X_Data_all,Data_X[subject]),axis = 0)\n",
    "        X_train_all = np.concatenate((X_train_all,Train_X[subject]),axis = 0)\n",
    "        X_test_all = np.concatenate((X_test_all,Test_X[subject]),axis = 0)\n",
    "\n",
    "        y_Data_all = np.concatenate((y_Data_all,Data_y[subject]),axis = 0)\n",
    "        y_train_all = np.concatenate((y_train_all,Train_y[subject]),axis = 0)\n",
    "        y_test_all = np.concatenate((y_test_all,Test_y[subject]),axis = 0)\n",
    "\n",
    "\n",
    "    print(X_Data_all.shape)\n",
    "    print(X_train_all.shape)\n",
    "    print(X_test_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remoing mean from data for all\n",
    "\n",
    "if (Process_data_each_subject==False):\n",
    "    Mean_all = {}     #  22 * 1000\n",
    "\n",
    "    Mean_all = np.mean(X_Data_all,axis = 0)   # mean across trials\n",
    "\n",
    "    X_Data_all -= Mean_all\n",
    "    X_train_all -= Mean_all                   # broadcating\n",
    "    X_test_all -= Mean_all\n",
    "\n",
    "    print(Mean_all.shape)\n",
    "    print(X_Data_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (Process_data_each_subject == False):\n",
    "\n",
    "    X_train_all = np.swapaxes(X_train_all,0,1)\n",
    "    X_train_all = np.reshape(X_train_all,(X_train_all.shape[0],X_train_all.shape[1]*Time_devide,X_train_all.shape[2]//Time_devide))\n",
    "    X_train_all = np.swapaxes(X_train_all,0,1)\n",
    "\n",
    "    X_test_all = np.swapaxes(X_test_all,0,1)\n",
    "    X_test_all = np.reshape(X_test_all,(X_test_all.shape[0],X_test_all.shape[1]*Time_devide,X_test_all.shape[2]//Time_devide))\n",
    "    X_test_all = np.swapaxes(X_test_all,0,1)\n",
    "\n",
    "    \n",
    "if (Process_data_each_subject == True):\n",
    "    \n",
    "    for i in np.arange(0,num_subject):\n",
    "        \n",
    "        Train_X[i+1] = np.swapaxes(Train_X[i+1],0,1)\n",
    "        Train_X[i+1] = np.reshape(Train_X[i+1],(Train_X[i+1].shape[0],Train_X[i+1].shape[1]*Time_devide,Train_X[i+1].shape[2]//Time_devide))\n",
    "        Train_X[i+1] = np.swapaxes(Train_X[i+1],0,1)\n",
    "        \n",
    "        Test_X[i+1] = np.swapaxes(Test_X[i+1],0,1)\n",
    "        Test_X[i+1] = np.reshape(Test_X[i+1],(Test_X[i+1].shape[0],Test_X[i+1].shape[1]*Time_devide,Test_X[i+1].shape[2]//Time_devide))\n",
    "        Test_X[i+1] = np.swapaxes(Test_X[i+1],0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (Process_data_each_subject == False):\n",
    "    y_train_all = np.repeat(y_train_all,Time_devide)\n",
    "    y_test_all = np.repeat(y_test_all,Time_devide)\n",
    "    \n",
    "    X_train_all = np.swapaxes(X_train_all,1,2)\n",
    "    X_test_all = np.swapaxes(X_test_all,1,2)\n",
    "    \n",
    "if (Process_data_each_subject == True):\n",
    "    for i in np.arange(0,num_subject):\n",
    "        \n",
    "        Train_y[i+1] = np.repeat(Train_y[i+1],Time_devide)\n",
    "        Test_y[i+1] = np.repeat(Test_y[i+1],Time_devide)\n",
    "    \n",
    "        Test_X[i+1] = np.swapaxes(Test_X[i+1],1,2)\n",
    "        Train_X[i+1] = np.swapaxes(Train_X[i+1],1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for training all / testing all\n",
    "\n",
    "if the the layer before last layer is Conv, flatten needs to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (Process_data_each_subject == False):\n",
    "    K.clear_session()    # clearing previous session\n",
    "\n",
    "    num_classes = 4\n",
    "    signal_length = 1000//Time_devide\n",
    "    number_channel = 22\n",
    "\n",
    "    input_shape = (signal_length,number_channel)  \n",
    "   \n",
    "    batch_size = X_train_all.shape[0]//batch_num\n",
    "\n",
    "    print('Loading data...')\n",
    "\n",
    "    x_train = X_train_all             # ? * 1000 * 22\n",
    "    y_train = y_train_all             # ?\n",
    "\n",
    "    x_test = X_test_all                 # ? * 1000 * 22\n",
    "    y_test = y_test_all                 # ?\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "\n",
    "    # # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    print(x_train.shape, 'train_x samples')\n",
    "    print(y_train.shape, 'train_y samples')\n",
    "    print(x_test.shape, 'test samples')\n",
    "\n",
    "    print('Build model...')\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    if (Architecture == 1):\n",
    "\n",
    "        # Conv\n",
    "        model.add(Conv1D(32, kernel_size = 11, input_shape=input_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides = 2))\n",
    "        model.add(Dropout(0.6))\n",
    "\n",
    "        # LSTM\n",
    "        model.add(LSTM(128, return_sequences=False))\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "    if (Architecture == 2):\n",
    "        \n",
    "        model.add(Conv1D(32, kernel_size = 11, input_shape=input_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides = 2))\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        model.add(Conv1D(32, kernel_size = 11))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides = 2))\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        model.add(Conv1D(32, kernel_size = 11))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides = 2))\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        # FC\n",
    "        model.add(Dense(64,activation = 'relu',input_shape=input_shape))\n",
    "    \n",
    "    if (Architecture == 3):\n",
    "\n",
    "        # Conv\n",
    "        model.add(Conv1D(64, kernel_size = 11, input_shape=input_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides = 2))\n",
    "        model.add(Dropout(0.6))\n",
    "        \n",
    "        model.add(Conv1D(64, kernel_size = 11))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides = 2))\n",
    "        model.add(Dropout(0.6))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        # FC\n",
    "        model.add(Dense(64,activation = 'relu',input_shape=input_shape))\n",
    "        \n",
    "    if (Architecture == 4):\n",
    "\n",
    "        # Conv\n",
    "        model.add(Conv1D(32, kernel_size = 11, input_shape=input_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides = 2))\n",
    "        model.add(Dropout(0.6))\n",
    "\n",
    "        # LSTM\n",
    "        model.add(LSTM(256, return_sequences=False))\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "    if (Architecture == 5):\n",
    "\n",
    "        # Conv\n",
    "        model.add(Conv1D(32, kernel_size = 11, input_shape=input_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides = 2))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        # LSTM\n",
    "        model.add(LSTM(128, return_sequences=False))\n",
    "        model.add(Dropout(0.5))\n",
    "    \n",
    "        model.add(Dense(64,activation = 'relu'))\n",
    "        \n",
    "    if (Architecture == 6):\n",
    "\n",
    "        # Conv\n",
    "        model.add(Conv1D(256, kernel_size = 11, input_shape=input_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides = 2))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        # LSTM\n",
    "        model.add(GRU(32, return_sequences=False))\n",
    "        model.add(Dropout(0.5))\n",
    "    \n",
    "        model.add(Dense(64,activation = 'relu'))  \n",
    "        \n",
    "        \n",
    "    # Output Layer\n",
    "    model.add(Dense(num_classes,activation = 'softmax'))\n",
    "\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    #optimizer = keras.optimizers.SGD(lr=1,decay=1e-6)\n",
    "    #optimizer = keras.optimizers.Adagrad(lr=0.001)\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print('Train...')\n",
    "\n",
    "    loss_train = []\n",
    "    acc_train = []\n",
    "\n",
    "    loss_val = []\n",
    "    acc_val = []\n",
    "\n",
    "    loss_test = []\n",
    "    acc_test = []\n",
    "\n",
    "\n",
    "    for i in np.arange(1,N_mont+1):\n",
    "        print('iteration',str(i)+'/'+str(N_mont))\n",
    "        \n",
    "        LIST=np.arange(x_train.shape[0])\n",
    "        Signal= np.arange(x_train.shape[1])\n",
    "        \n",
    "        np.random.shuffle(LIST)\n",
    "        #np.random.shuffle(Signal)\n",
    "        \n",
    "        y_train=y_train[LIST]\n",
    "        \n",
    "        x_train =  x_train[LIST] \n",
    "        #x_train1 = np.swapaxes(x_train1,0,1) \n",
    "        #x_train1 =  x_train1[Signal]\n",
    "        #x_train1 = np.swapaxes(x_train1,0,1)  \n",
    "        \n",
    "        #x_test1 = np.swapaxes(x_test,0,1) \n",
    "        #x_test1 =  x_test1[Signal]\n",
    "        #x_test1 = np.swapaxes(x_test1,0,1)  \n",
    "        \n",
    "        history = model.fit(x_train, y_train,\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=epochs,\n",
    "                 verbose=1,\n",
    "                 validation_split = 0.1)\n",
    "\n",
    "        loss_train.append(np.mean(history.history['loss']))\n",
    "        acc_train.append(np.mean(history.history['acc']))\n",
    "\n",
    "        loss_val.append(np.mean(history.history['val_loss']))\n",
    "        acc_val.append(np.mean(history.history['val_acc']))\n",
    "\n",
    "        score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "        loss_test.append(score[0])\n",
    "        acc_test.append(score[1])\n",
    "\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training for one subject / Test for each and all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table_training = {}\n",
    "data_table_test = {}\n",
    "data_table_val = {}\n",
    "\n",
    "if (Process_data_each_subject == True):\n",
    "    K.clear_session()    # clearing previous session\n",
    "    \n",
    "    num_classes = 4\n",
    "    signal_length = 1000//Time_devide\n",
    "    number_channel = 22\n",
    "\n",
    "    input_shape = (signal_length,number_channel)  \n",
    "\n",
    "    batch_size = 200//batch_num\n",
    "\n",
    "    print('Loading data...')\n",
    "    \n",
    "    x_train = Train_X[Subject_train]             # 13344 * 125 * 22\n",
    "    y_train = Train_y[Subject_train]            # 13344\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "\n",
    "    # # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "\n",
    "\n",
    "    print('Build model...')\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    if (Architecture == 1):\n",
    "\n",
    "        # Conv\n",
    "        model.add(Conv1D(32, kernel_size = 11, input_shape=input_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides = 2))\n",
    "        model.add(Dropout(0.6))\n",
    "\n",
    "        # LSTM\n",
    "        model.add(LSTM(128, return_sequences=False))\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "    if (Architecture == 2):\n",
    "        \n",
    "        model.add(Conv1D(32, kernel_size = 11, input_shape=input_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides = 2))\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        model.add(Conv1D(32, kernel_size = 11))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides = 2))\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        model.add(Conv1D(32, kernel_size = 11))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides = 2))\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        # FC\n",
    "        model.add(Dense(64,activation = 'relu'))\n",
    "    \n",
    "    if (Architecture == 3):\n",
    "\n",
    "        # Conv\n",
    "        model.add(Conv1D(64, kernel_size = 11, input_shape=input_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides = 2))\n",
    "        model.add(Dropout(0.6))\n",
    "        \n",
    "        model.add(Conv1D(64, kernel_size = 11))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides = 2))\n",
    "        model.add(Dropout(0.6))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        # FC\n",
    "        model.add(Dense(64,activation = 'relu'))\n",
    "        \n",
    "    if (Architecture == 4):\n",
    "\n",
    "        # Conv\n",
    "        model.add(Conv1D(32, kernel_size = 11, input_shape=input_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides = 2))\n",
    "        model.add(Dropout(0.6))\n",
    "\n",
    "        # LSTM\n",
    "        model.add(LSTM(256, return_sequences=False))\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "    if (Architecture == 5):\n",
    "\n",
    "        # Conv\n",
    "        model.add(Conv1D(32, kernel_size = 11, input_shape=input_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides = 2))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        # LSTM\n",
    "        model.add(LSTM(128, return_sequences=False))\n",
    "        model.add(Dropout(0.5))\n",
    "    \n",
    "        model.add(Dense(64,activation = 'relu'))\n",
    "        \n",
    "    if (Architecture == 6):\n",
    "\n",
    "        # Conv\n",
    "        model.add(Conv1D(256, kernel_size = 11, input_shape=input_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides = 2))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        # LSTM\n",
    "        model.add(GRU(32, return_sequences=False))\n",
    "        model.add(Dropout(0.5))\n",
    "    \n",
    "        model.add(Dense(64,activation = 'relu'))  \n",
    "\n",
    "\n",
    "    # # Output Layer\n",
    "    model.add(Dense(num_classes,activation = 'softmax'))\n",
    "\n",
    "    # # try using different optimizers and different optimizer configs\n",
    "    optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "#     optimizer = keras.optimizers.SGD(lr=1,decay=1e-6)\n",
    "    #optimizer = keras.optimizers.Adagrad(lr=0.001)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print('Train...')\n",
    "\n",
    "    loss_train = []\n",
    "    acc_train = []\n",
    "\n",
    "    loss_val = []\n",
    "    acc_val = []\n",
    "\n",
    "    loss_test = []\n",
    "    acc_test = []\n",
    "\n",
    "\n",
    "    for i_test in np.arange(1,num_subject+1):\n",
    "        \n",
    "        print('Training Subject: ', Subject_train , 'Testing Subject: ', i_test)\n",
    "        x_test = Test_X[i_test]                 # 3600, 125, 22\n",
    "        y_test = Test_y[i_test]                 # 3600\n",
    "        \n",
    "        y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "        x_test = x_test.astype('float32')\n",
    "        \n",
    "        for i in np.arange(1,N_mont):\n",
    "            LIST=np.arange(x_train.shape[0])\n",
    "            np.random.shuffle(LIST)\n",
    "            x_train=x_train[LIST]\n",
    "            y_train=y_train[LIST]\n",
    "            \n",
    "            history = model.fit(x_train, y_train,\n",
    "                     batch_size=batch_size,\n",
    "                     epochs=epochs,\n",
    "                     verbose=0,\n",
    "                     validation_split = 0.1)\n",
    "\n",
    "            loss_train.append([Subject_train,i_test,np.mean(history.history['loss'])])\n",
    "            acc_train.append([Subject_train,i_test,np.mean(history.history['acc'])])\n",
    "            loss_val.append([Subject_train,i_test,np.mean(history.history['val_loss'])])\n",
    "            acc_val.append([Subject_train,i_test,np.mean(history.history['val_acc'])])\n",
    "\n",
    "            score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "            loss_test.append([Subject_train,i_test,score[0]])\n",
    "            acc_test.append([Subject_train,i_test,score[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (Process_data_each_subject == True):\n",
    "    loss_val_arr=np.array(loss_val)\n",
    "    loss_val_avg=[]\n",
    "    m=0\n",
    "    for ii in np.arange(1,num_subject+1):\n",
    "        avg=(np.mean(loss_val_arr[0+m:N_mont-1+m],axis=0)[2])\n",
    "        loss_val_avg.append(avg)\n",
    "        m=m+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (Process_data_each_subject == True):\n",
    "    acc_test_arr=np.array(acc_test)\n",
    "    acc_test_avg=[]\n",
    "    m=0\n",
    "    avg=0\n",
    "    for ii in np.arange(1,num_subject+1):\n",
    "        avg=(np.max(acc_test_arr[0+m:N_mont-1+m],axis=0)[2])\n",
    "        acc_test_avg.append(avg)\n",
    "        m=m+2\n",
    "\n",
    "\n",
    "    print(acc_test_avg)\n",
    "    print(np.mean(acc_test_avg))\n",
    "    print(np.max(acc_test_avg))\n",
    "    print(np.min(acc_test_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (Process_data_each_subject == True):\n",
    "    acc_test_arr=np.array(acc_test)\n",
    "    acc_train_arr=np.array(acc_train)\n",
    "    acc_val_arr=np.array(acc_val)\n",
    "    acc_test_avg=[]\n",
    "    acc_train_avg=[]\n",
    "    acc_val_avg=[]\n",
    "    m=0\n",
    "    avg=0\n",
    "    for ii in np.arange(1,num_subject+1):\n",
    "        avg_test=(np.max(acc_test_arr[0+m:N_mont-1+m],axis=0)[2])\n",
    "        avg_train=(np.max(acc_train_arr[0+m:N_mont-1+m],axis=0)[2])\n",
    "        avg_val=(np.max(acc_val_arr[0+m:N_mont-1+m],axis=0)[2])\n",
    "        acc_test_avg.append(avg_test)\n",
    "        acc_val_avg.append(avg_val)\n",
    "        acc_train_avg.append(avg_train)\n",
    "        m=m+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (Process_data_each_subject == True):\n",
    "    print(acc_train_avg)\n",
    "    print(acc_test_avg)\n",
    "    print(acc_val_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (Process_data_each_subject == True):\n",
    "    min_acc_train = np.min(acc_train_avg)\n",
    "    max_acc_train = np.max(acc_train_avg)\n",
    "    mean_acc_train = np.mean(acc_train_avg)\n",
    "\n",
    "    min_acc_test = np.min(acc_test_avg)\n",
    "    max_acc_test = np.max(acc_test_avg)\n",
    "    mean_acc_test = np.mean(acc_test_avg)\n",
    "\n",
    "    min_acc_val = np.min(acc_val_avg)\n",
    "    max_acc_val = np.max(acc_val_avg)\n",
    "    mean_acc_val = np.mean(acc_val_avg)\n",
    "\n",
    "    data_table_training[str(Subject_train)] = [min_acc_train,max_acc_train,mean_acc_train]\n",
    "    data_table_test[str(Subject_train)] = [min_acc_test,max_acc_test,mean_acc_test] \n",
    "    data_table_val[str(Subject_train)] = [min_acc_val,max_acc_val,mean_acc_val]\n",
    "\n",
    "    print('Training across subject:', Subject_train)\n",
    "    print('Min acc:', min_acc_train,'Max acc:',max_acc_train, 'Mean acc:', mean_acc_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
